{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141f490d-f724-435f-8361-2bed62723580",
   "metadata": {},
   "source": [
    "TOPIC: Understanding Pooling and Padding in CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a3c40f-4eae-4f48-a9ac-580419957665",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 1\n",
    "   \n",
    "Purpose of Pooling:\n",
    "    \n",
    "Dimensionality Reduction: Pooling layers reduce the spatial dimensions (width and height) of the input,\n",
    "which helps in decreasing the computational load and the number of parameters, thus mitigating the risk of \n",
    "overfitting.\n",
    "\n",
    "Translation Invariance: By summarizing the presence of features in the pooled region, pooling layers contribute\n",
    "to making the CNN less sensitive to the exact position of features within an image.\n",
    "\n",
    "Feature Extraction: Pooling helps in extracting dominant features, which makes it easier for subsequent layers \n",
    "to process essential information without getting bogged down by details.\n",
    "\n",
    "Benefits of Pooling:\n",
    "\n",
    "Reduces Overfitting: By reducing the dimensionality and the number of parameters, pooling helps in preventing \n",
    "overfitting, making the model generalize better to new, unseen data.\n",
    "\n",
    "Computational Efficiency: Smaller input dimensions lead to faster computations, which means quicker training \n",
    "times and lower resource usage.\n",
    "\n",
    "Robustness to Variations: Pooling makes the CNN more robust to variations such as minor translations and\n",
    "distortions in the input images, improving the model's ability to recognize objects regardless of slight changes in position or appearance.\n",
    "\n",
    "Combines Features: Pooling combines features from the convolutional layers, which helps in creating a hierarchical\n",
    "structure of features, enhancing the model's ability to understand complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4016e2-fe9d-4921-b091-0075989c4b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 2\n",
    "   \n",
    "### Max Pooling:\n",
    "- **Functionality**: Max pooling selects the maximum value from the input region (a sub-region of the feature map).\n",
    "- **Purpose**: It captures the most prominent feature within that region, emphasizing strong activations and important features.\n",
    "- **Effect**: Enhances the model's ability to recognize significant patterns, but can be sensitive to noise if a single high value represents noise rather than a meaningful feature.\n",
    "- **Formula**: For a given region \\((i, j)\\) in the feature map, the output is:\n",
    "  \\[\n",
    "  \\text{MaxPooling}(i, j) = \\max(\\text{Region}(i, j))\n",
    "  \\]\n",
    "\n",
    "### Min Pooling:\n",
    "- **Functionality**: Min pooling selects the minimum value from the input region.\n",
    "- **Purpose**: It focuses on the least activated feature within that region.\n",
    "- **Effect**: Can be useful in certain applications where detecting the weakest signal or the lowest intensity is important, but is less common in practice compared to max pooling.\n",
    "- **Formula**: For a given region \\((i, j)\\) in the feature map, the output is:\n",
    "  \\[\n",
    "  \\text{MinPooling}(i, j) = \\min(\\text{Region}(i, j))\n",
    "  \\]\n",
    "\n",
    "### Key Differences:\n",
    "1. **Focus**:\n",
    "   - **Max Pooling**: Highlights the most significant features, making it ideal for tasks where the presence of strong features is crucial.\n",
    "   - **Min Pooling**: Highlights the weakest features, which can be useful in specific contexts but is less commonly used in standard CNN architectures.\n",
    "\n",
    "2. **Common Usage**:\n",
    "   - **Max Pooling**: Widely used in most CNNs due to its effectiveness in capturing important patterns and reducing dimensionality while retaining critical information.\n",
    "   - **Min Pooling**: Rarely used in practice; mostly found in niche applications where detecting low-intensity features is necessary.\n",
    "\n",
    "3. **Impact on the Feature Map**:\n",
    "   - **Max Pooling**: Produces a feature map that emphasizes high-activation regions, which often correspond to significant features in the input.\n",
    "   - **Min Pooling**: Produces a feature map that emphasizes low-activation regions, which might correspond to background or less relevant parts of the input.\n",
    "\n",
    "In summary, while both pooling methods aim to reduce the dimensionality of the input feature maps, max pooling is more commonly used due to its ability to capture and emphasize the most relevant features in an image, making it more suitable for a wide range of applications in computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5079585-4839-4d53-8a32-877e329ef992",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 3\n",
    "   \n",
    "Padding in CNNs is the technique of adding extra pixels around the input image or feature map to maintain\n",
    "spatial dimensions during the convolution operation. It helps prevent information loss at the edges and plays \n",
    "a vital role in the architecture and performance of convolutional neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e56aa1-9b4b-4362-a792-11922f003c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 4\n",
    "   \n",
    "Zero-padding and valid-padding are techniques used in convolutional neural networks (CNNs) that affect the size of the output feature map. Here's a detailed comparison:\n",
    "\n",
    "### Zero-Padding\n",
    "\n",
    "**Definition:**  \n",
    "Zero-padding involves adding zeros around the border of the input image before applying the convolution operation. This padding can help preserve the spatial dimensions of the input.\n",
    "\n",
    "**Effect on Output Feature Map Size:**\n",
    "- **Formula:** If the input size is \\( N \\times N \\), the filter size is \\( F \\times F \\), the stride is \\( S \\), and the padding is \\( P \\), the output size \\( O \\) is given by:\n",
    "  \\[\n",
    "  O = \\frac{N - F + 2P}{S} + 1\n",
    "  \\]\n",
    "- **Preservation of Size:** By choosing \\( P \\) appropriately, zero-padding can preserve the input size. For example, with a stride \\( S = 1 \\) and a filter size \\( F = 3 \\), setting \\( P = 1 \\) results in an output size equal to the input size.\n",
    "\n",
    "**Advantages:**\n",
    "- **Preservation of spatial dimensions:** Helps maintain the size of the input feature map.\n",
    "- **Edge features:** Allows edge features to be captured more effectively.\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Computational overhead:** Additional padding adds to computational cost.\n",
    "- **Potential for unnecessary features:** May introduce unnecessary features by padding with zeros.\n",
    "\n",
    "### Valid-Padding\n",
    "\n",
    "**Definition:**  \n",
    "Valid-padding, also known as \"no padding,\" means that the convolution operation is only applied to the parts of the input where the filter completely fits, without adding any extra pixels around the border.\n",
    "\n",
    "**Effect on Output Feature Map Size:**\n",
    "- **Formula:** If the input size is \\( N \\times N \\), the filter size is \\( F \\times F \\), and the stride is \\( S \\), the output size \\( O \\) is given by:\n",
    "  \\[\n",
    "  O = \\frac{N - F}{S} + 1\n",
    "  \\]\n",
    "- **Reduction in Size:** The output size is always smaller than the input size unless the filter size is 1 and the stride is 1.\n",
    "\n",
    "**Advantages:**\n",
    "- **Simplicity:** No need to add extra pixels, making it computationally simpler.\n",
    "- **Avoidance of unnecessary features:** No introduction of artificial features from padding.\n",
    "\n",
    "**Disadvantages:**\n",
    "- **Loss of information:** Edge information can be lost as the filter does not cover the borders completely.\n",
    "- **Reduction in spatial dimensions:** Output feature map size is reduced, which might not be desirable for certain applications.\n",
    "\n",
    "### Summary of Differences\n",
    "\n",
    "| Aspect                  | Zero-Padding                              | Valid-Padding                             |\n",
    "|-------------------------|-------------------------------------------|-------------------------------------------|\n",
    "| **Definition**          | Adding zeros around the input             | No padding                                |\n",
    "| **Output Size Formula** | \\(\\frac{N - F + 2P}{S} + 1\\)              | \\(\\frac{N - F}{S} + 1\\)                   |\n",
    "| **Output Size Effect**  | Can preserve input size (with appropriate \\(P\\)) | Output size is always smaller             |\n",
    "| **Edge Information**    | Retained                                  | Can be lost                               |\n",
    "| **Computational Cost**  | Higher due to added pixels                | Lower                                     |\n",
    "| **Feature Extraction**  | May introduce artificial features         | Extracts only relevant features           |\n",
    "\n",
    "In conclusion, the choice between zero-padding and valid-padding depends on the specific requirements of the task. Zero-padding is useful when preserving spatial dimensions and edge features is important, while valid-padding is preferred for a more straightforward, no-extra-features approach, albeit at the cost of losing some border information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c1eb48-683c-4f2a-b73a-ba19b9318855",
   "metadata": {},
   "source": [
    "TOPIC: Exploring LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a0bb3b-9f0c-4d1b-890d-430b8b3d49eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 1\n",
    "    \n",
    "LeNet-5 is a pioneering convolutional neural network (CNN) architecture designed by Yann LeCun et al. in 1998, primarily for handwritten digit recognition (MNIST dataset). It laid the foundation for many modern deep learning models. Here is a detailed overview of the LeNet-5 architecture:\n",
    "\n",
    "### Architecture Overview\n",
    "\n",
    "LeNet-5 consists of seven layers, excluding the input layer. These layers include three convolutional layers, two subsampling (pooling) layers, and two fully connected layers, followed by a final output layer.\n",
    "\n",
    "### Layer-by-Layer Description\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - **Size:** 32x32 pixels, grayscale image.\n",
    "   - **Purpose:** Preprocesses and normalizes the input image. The original MNIST images (28x28) are padded to 32x32 to allow for easier handling of the edge pixels.\n",
    "\n",
    "2. **Layer C1: Convolutional Layer**\n",
    "   - **Number of Filters:** 6\n",
    "   - **Filter Size:** 5x5\n",
    "   - **Stride:** 1\n",
    "   - **Output Size:** 28x28x6 (since \\(32 - 5 + 1 = 28\\))\n",
    "   - **Activation Function:** tanh\n",
    "   - **Purpose:** Extracts local features such as edges and textures.\n",
    "\n",
    "3. **Layer S2: Subsampling (Pooling) Layer**\n",
    "   - **Type:** Average pooling\n",
    "   - **Filter Size:** 2x2\n",
    "   - **Stride:** 2\n",
    "   - **Output Size:** 14x14x6 (since \\(28 / 2 = 14\\))\n",
    "   - **Purpose:** Reduces dimensionality and retains important features, providing translation invariance.\n",
    "\n",
    "4. **Layer C3: Convolutional Layer**\n",
    "   - **Number of Filters:** 16\n",
    "   - **Filter Size:** 5x5\n",
    "   - **Stride:** 1\n",
    "   - **Output Size:** 10x10x16 (since \\(14 - 5 + 1 = 10\\))\n",
    "   - **Activation Function:** tanh\n",
    "   - **Purpose:** Extracts more complex features by combining the simple features learned in C1.\n",
    "\n",
    "5. **Layer S4: Subsampling (Pooling) Layer**\n",
    "   - **Type:** Average pooling\n",
    "   - **Filter Size:** 2x2\n",
    "   - **Stride:** 2\n",
    "   - **Output Size:** 5x5x16 (since \\(10 / 2 = 5\\))\n",
    "   - **Purpose:** Further reduces dimensionality and retains essential features, enhancing translation invariance.\n",
    "\n",
    "6. **Layer C5: Convolutional Layer**\n",
    "   - **Number of Filters:** 120\n",
    "   - **Filter Size:** 5x5\n",
    "   - **Stride:** 1\n",
    "   - **Output Size:** 1x1x120 (since \\(5 - 5 + 1 = 1\\))\n",
    "   - **Activation Function:** tanh\n",
    "   - **Purpose:** Fully connected to the previous layer, combining all features into a 1x1 output per filter.\n",
    "\n",
    "7. **Layer F6: Fully Connected Layer**\n",
    "   - **Number of Neurons:** 84\n",
    "   - **Activation Function:** tanh\n",
    "   - **Purpose:** Acts as a traditional fully connected neural network layer, performing classification by integrating features from the previous layer.\n",
    "\n",
    "8. **Output Layer:**\n",
    "   - **Number of Neurons:** 10 (one for each digit class)\n",
    "   - **Activation Function:** Softmax (not explicitly stated in the original paper, but common practice in modern implementations)\n",
    "   - **Purpose:** Produces probability distributions over the 10 digit classes for classification.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Input Size:** 32x32 grayscale image\n",
    "- **Architecture:** \n",
    "  1. Convolution (C1): 6x28x28\n",
    "  2. Pooling (S2): 6x14x14\n",
    "  3. Convolution (C3): 16x10x10\n",
    "  4. Pooling (S4): 16x5x5\n",
    "  5. Convolution (C5): 120x1x1\n",
    "  6. Fully Connected (F6): 84\n",
    "  7. Output: 10 (classes)\n",
    "\n",
    "LeNet-5's design introduced many key concepts used in CNNs today, such as alternating convolutional and pooling layers, and using fully connected layers at the end for classification. Its simplicity and effectiveness make it a classic example in the field of deep learning.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc75a0ab-ef5c-460b-9bc3-cdf0e9bc86a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 2\n",
    "   \n",
    "Key Components and Purposes\n",
    "Input Layer\n",
    "\n",
    "Size: 32x32 pixels, grayscale image.\n",
    "Purpose: Preprocesses and normalizes the input image. The padding to 32x32 allows the network to handle edge pixels more effectively.\n",
    "\n",
    "Layer C1: Convolutional Layer\n",
    "\n",
    "Number of Filters: 6\n",
    "Filter Size: 5x5\n",
    "Output Size: 28x28x6\n",
    "Activation Function: tanh\n",
    "Purpose: Extracts local features such as edges and textures from the input image by applying convolution operations.\n",
    "\n",
    "Layer S2: Subsampling (Pooling) Layer\n",
    "\n",
    "Type: Average pooling\n",
    "Filter Size: 2x2\n",
    "Stride: 2\n",
    "Output Size: 14x14x6\n",
    "Purpose: Reduces the spatial dimensions by downsampling the feature maps, which helps in achieving translation invariance and reducing computational complexity.\n",
    "\n",
    "Layer C3: Convolutional Layer\n",
    "\n",
    "Number of Filters: 16\n",
    "Filter Size: 5x5\n",
    "Output Size: 10x10x16\n",
    "Activation Function: tanh\n",
    "Purpose: Extracts more complex features by combining simple features learned in the previous layer. This layer connects different combinations of the input feature maps.\n",
    "\n",
    "Layer S4: Subsampling (Pooling) Layer\n",
    "\n",
    "Type: Average pooling\n",
    "Filter Size: 2x2\n",
    "Stride: 2\n",
    "Output Size: 5x5x16\n",
    "Purpose: Further reduces the spatial dimensions and retains the most important features, enhancing translation invariance and reducing the computational load.\n",
    "\n",
    "Layer C5: Convolutional Layer\n",
    "\n",
    "Number of Filters: 120\n",
    "Filter Size: 5x5\n",
    "Output Size: 1x1x120\n",
    "Activation Function: tanh\n",
    "Purpose: Acts as a fully connected layer with each unit connected to all units of the previous layer. It combines all the extracted features into a dense representation.\n",
    "\n",
    "Layer F6: Fully Connected Layer\n",
    "\n",
    "Number of Neurons: 84\n",
    "Activation Function: tanh\n",
    "Purpose: Acts as a traditional fully connected layer, performing further abstraction and integration of the features. It prepares the features for the final classification.\n",
    "Output Layer\n",
    "\n",
    "Number of Neurons: 10 (one for each digit class)\n",
    "Activation Function: Softmax (commonly used in modern implementations)\n",
    "Purpose: Produces a probability distribution over the 10 digit classes, enabling the classification of the input image into one of the digit categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4023a18-d72e-41e8-a07f-78961d48ef4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 3\n",
    "   \n",
    "LeNet-5, designed by Yann LeCun et al. in 1998, was one of the first successful convolutional neural networks (CNNs) and has significantly influenced modern deep learning. Here's a discussion of its advantages and limitations in the context of image classification tasks:\n",
    "\n",
    "### Advantages of LeNet-5\n",
    "\n",
    "1. **Pioneering Design:**\n",
    "   - **Innovation:** LeNet-5 introduced key concepts like convolutional layers, pooling layers, and the use of the tanh activation function, which laid the groundwork for future CNN architectures.\n",
    "   - **Foundation:** It served as a foundational model that inspired subsequent, more advanced architectures like AlexNet, VGG, and ResNet.\n",
    "\n",
    "2. **Hierarchical Feature Extraction:**\n",
    "   - **Layer-wise Feature Learning:** By using multiple convolutional and pooling layers, LeNet-5 can learn hierarchical representations of the input data, capturing simple features like edges in early layers and more complex patterns in deeper layers.\n",
    "   - **Effectiveness:** This hierarchical learning makes LeNet-5 effective for image classification tasks, especially those involving handwritten digits or simple images.\n",
    "\n",
    "3. **Parameter Efficiency:**\n",
    "   - **Shared Weights:** Convolutional layers use shared weights, reducing the number of parameters compared to fully connected networks and making the model more computationally efficient.\n",
    "   - **Pooling Layers:** Subsampling (pooling) layers help reduce the dimensionality of feature maps, further decreasing computational requirements and the risk of overfitting.\n",
    "\n",
    "4. **Translation Invariance:**\n",
    "   - **Pooling Layers:** Pooling layers provide a degree of translation invariance, allowing the network to recognize objects regardless of their position in the input image.\n",
    "\n",
    "5. **Training Stability:**\n",
    "   - **Normalization:** Preprocessing steps and normalization of input data help stabilize training and improve convergence.\n",
    "\n",
    "### Limitations of LeNet-5\n",
    "\n",
    "1. **Limited Complexity:**\n",
    "   - **Simple Architecture:** LeNet-5's architecture is relatively simple and may not capture the complexity needed for modern, high-resolution image classification tasks.\n",
    "   - **Small Depth:** With only a few layers, LeNet-5 is less capable of learning deep, abstract features compared to deeper networks.\n",
    "\n",
    "2. **Restricted Input Size:**\n",
    "   - **Fixed Input Dimensions:** LeNet-5 is designed for 32x32 pixel inputs, which limits its direct applicability to larger, high-resolution images without significant modifications or preprocessing.\n",
    "   - **Specific Use Case:** It is primarily tailored for tasks like handwritten digit recognition (e.g., MNIST dataset) and may not generalize well to diverse image datasets without adaptation.\n",
    "\n",
    "3. **Activation Functions:**\n",
    "   - **Tanh Activation:** LeNet-5 uses the tanh activation function, which can suffer from vanishing gradient problems, slowing down training and reducing model performance. Modern architectures often use ReLU (Rectified Linear Unit) for better gradient flow and faster convergence.\n",
    "\n",
    "4. **Lack of Regularization Techniques:**\n",
    "   - **No Dropout:** LeNet-5 does not incorporate modern regularization techniques like dropout, which helps prevent overfitting in more complex models.\n",
    "   - **Limited Data Augmentation:** It lacks advanced data augmentation strategies that are commonly used today to improve model robustness.\n",
    "\n",
    "5. **Scalability:**\n",
    "   - **Not Scalable:** The design of LeNet-5 is not easily scalable to much larger and more complex datasets, such as ImageNet, which require deeper architectures with more parameters and sophisticated techniques.\n",
    "\n",
    "### Summary\n",
    "\n",
    "#### Advantages:\n",
    "- **Pioneering design with key CNN concepts.**\n",
    "- **Effective hierarchical feature extraction.**\n",
    "- **Parameter efficiency through shared weights.**\n",
    "- **Translation invariance from pooling layers.**\n",
    "- **Stable training due to normalization.**\n",
    "\n",
    "#### Limitations:\n",
    "- **Limited complexity and depth.**\n",
    "- **Restricted to small input sizes.**\n",
    "- **Potential vanishing gradient issues with tanh activation.**\n",
    "- **Lack of modern regularization techniques.**\n",
    "- **Not easily scalable to larger, more complex tasks.**\n",
    "\n",
    "In conclusion, while LeNet-5 was groundbreaking and remains influential, its simplicity and design limitations make it less suitable for modern, large-scale image classification tasks. Advances in deep learning have led to more complex and powerful architectures that address these limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68b6a2ec-87b4-4812-9687-61d4c94e41b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60216a90-1a17-46f6-82cd-7c031b6e46b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.19.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m70.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting filelock\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (2.8.8)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch) (2022.11.0)\n",
      "Collecting typing-extensions>=4.8.0\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Collecting triton==3.0.0\n",
      "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision) (1.23.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision) (9.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.2.1)\n",
      "Installing collected packages: typing-extensions, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "Successfully installed filelock-3.15.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 torchvision-0.19.0 triton-3.0.0 typing-extensions-4.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49a2fae-9a99-4bec-b081-ec62d1c295ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the LeNet-5 model\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations for the training and testing data\n",
    "transform = transforms.Compose([transforms.Resize((32, 32)), transforms.ToTensor()])\n",
    "\n",
    "# Load MNIST dataset\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Instantiate the model, define the loss function and the optimizer\n",
    "model = LeNet5().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train(model, device, trainloader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(trainloader.dataset)} ({100. * batch_idx / len(trainloader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "# Testing function\n",
    "def test(model, device, testloader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    accuracy = 100. * correct / len(testloader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(testloader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "    return accuracy\n",
    "\n",
    "# Train and test the model\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, trainloader, optimizer, criterion, epoch)\n",
    "    test(model, device, testloader, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e901a960-3e58-40f7-aa59-35f2795e680f",
   "metadata": {},
   "source": [
    "TOPIC: Analyzing AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49182899-9ccf-4635-a981-79a7b96faac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 1\n",
    "   \n",
    "AlexNet, introduced by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton in 2012, was a groundbreaking convolutional neural network (CNN) that won the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 by a significant margin. AlexNet demonstrated the power of deep learning in computer vision and influenced the design of many subsequent CNN architectures. Here is an overview of the AlexNet architecture:\n",
    "\n",
    "### Key Components of AlexNet\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - **Size:** 224x224x3 RGB image.\n",
    "   - **Purpose:** Standardizes input image size for processing by the network.\n",
    "\n",
    "2. **Convolutional Layers:**\n",
    "   - **Layer 1 (Conv1):**\n",
    "     - **Filters:** 96\n",
    "     - **Filter Size:** 11x11\n",
    "     - **Stride:** 4\n",
    "     - **Padding:** 0\n",
    "     - **Output:** 55x55x96\n",
    "     - **Activation:** ReLU\n",
    "   - **Layer 2 (Conv2):**\n",
    "     - **Filters:** 256\n",
    "     - **Filter Size:** 5x5\n",
    "     - **Stride:** 1\n",
    "     - **Padding:** 2\n",
    "     - **Output:** 27x27x256\n",
    "     - **Activation:** ReLU\n",
    "   - **Layer 3 (Conv3):**\n",
    "     - **Filters:** 384\n",
    "     - **Filter Size:** 3x3\n",
    "     - **Stride:** 1\n",
    "     - **Padding:** 1\n",
    "     - **Output:** 13x13x384\n",
    "     - **Activation:** ReLU\n",
    "   - **Layer 4 (Conv4):**\n",
    "     - **Filters:** 384\n",
    "     - **Filter Size:** 3x3\n",
    "     - **Stride:** 1\n",
    "     - **Padding:** 1\n",
    "     - **Output:** 13x13x384\n",
    "     - **Activation:** ReLU\n",
    "   - **Layer 5 (Conv5):**\n",
    "     - **Filters:** 256\n",
    "     - **Filter Size:** 3x3\n",
    "     - **Stride:** 1\n",
    "     - **Padding:** 1\n",
    "     - **Output:** 13x13x256\n",
    "     - **Activation:** ReLU\n",
    "\n",
    "3. **Pooling Layers:**\n",
    "   - **Layer 1 (Pool1):**\n",
    "     - **Type:** Max pooling\n",
    "     - **Filter Size:** 3x3\n",
    "     - **Stride:** 2\n",
    "     - **Output:** 27x27x96\n",
    "   - **Layer 2 (Pool2):**\n",
    "     - **Type:** Max pooling\n",
    "     - **Filter Size:** 3x3\n",
    "     - **Stride:** 2\n",
    "     - **Output:** 13x13x256\n",
    "   - **Layer 3 (Pool3):**\n",
    "     - **Type:** Max pooling\n",
    "     - **Filter Size:** 3x3\n",
    "     - **Stride:** 2\n",
    "     - **Output:** 6x6x256\n",
    "\n",
    "4. **Normalization Layers:**\n",
    "   - **Local Response Normalization (LRN):** Applied after Conv1 and Conv2 to improve generalization and reduce the effect of ReLU’s activation.\n",
    "\n",
    "5. **Fully Connected Layers:**\n",
    "   - **FC6:**\n",
    "     - **Neurons:** 4096\n",
    "     - **Activation:** ReLU\n",
    "   - **FC7:**\n",
    "     - **Neurons:** 4096\n",
    "     - **Activation:** ReLU\n",
    "\n",
    "6. **Output Layer:**\n",
    "   - **FC8:**\n",
    "     - **Neurons:** 1000 (one for each class in ImageNet)\n",
    "     - **Activation:** Softmax\n",
    "\n",
    "### Detailed Architecture\n",
    "\n",
    "1. **Input:** 224x224x3 RGB image.\n",
    "2. **Conv1:** 96 filters, 11x11 kernel, stride 4, output 55x55x96, ReLU.\n",
    "3. **LRN:** Local response normalization.\n",
    "4. **Pool1:** Max pooling, 3x3 kernel, stride 2, output 27x27x96.\n",
    "5. **Conv2:** 256 filters, 5x5 kernel, stride 1, padding 2, output 27x27x256, ReLU.\n",
    "6. **LRN:** Local response normalization.\n",
    "7. **Pool2:** Max pooling, 3x3 kernel, stride 2, output 13x13x256.\n",
    "8. **Conv3:** 384 filters, 3x3 kernel, stride 1, padding 1, output 13x13x384, ReLU.\n",
    "9. **Conv4:** 384 filters, 3x3 kernel, stride 1, padding 1, output 13x13x384, ReLU.\n",
    "10. **Conv5:** 256 filters, 3x3 kernel, stride 1, padding 1, output 13x13x256, ReLU.\n",
    "11. **Pool3:** Max pooling, 3x3 kernel, stride 2, output 6x6x256.\n",
    "12. **Flatten:** Flatten the output for the fully connected layers.\n",
    "13. **FC6:** Fully connected layer with 4096 neurons, ReLU.\n",
    "14. **Dropout:** Dropout with a probability of 0.5 to reduce overfitting.\n",
    "15. **FC7:** Fully connected layer with 4096 neurons, ReLU.\n",
    "16. **Dropout:** Dropout with a probability of 0.5 to reduce overfitting.\n",
    "17. **FC8:** Fully connected layer with 1000 neurons, Softmax activation for classification.\n",
    "\n",
    "### Innovations and Contributions\n",
    "\n",
    "1. **ReLU Activation:** Introduced ReLU (Rectified Linear Unit) activation, which accelerates the training process compared to traditional sigmoid or tanh activations.\n",
    "2. **GPU Utilization:** Made extensive use of GPUs to train the model, demonstrating the importance of GPU acceleration in deep learning.\n",
    "3. **Local Response Normalization (LRN):** Introduced LRN to help generalize better by normalizing the activations.\n",
    "4. **Dropout:** Employed dropout in the fully connected layers to mitigate overfitting.\n",
    "\n",
    "### Performance\n",
    "\n",
    "- **ILSVRC 2012:** Achieved a top-5 error rate of 15.3%, significantly outperforming the runner-up with an error rate of 26.2%.\n",
    "- **Impact:** AlexNet's success demonstrated the effectiveness of deep CNNs and sparked a surge in deep learning research and applications, particularly in computer vision.\n",
    "\n",
    "### Summary\n",
    "\n",
    "AlexNet's architecture and innovations were crucial in proving the potential of deep learning for image classification tasks. Its use of ReLU activation, GPU acceleration, LRN, and dropout were key factors in its success and have become standard practices in modern CNN architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e8b10-a5e9-42e5-873d-bf02f4680bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 2\n",
    "   \n",
    "AlexNet introduced several architectural innovations that significantly contributed to its performance in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012. These innovations helped the model achieve a top-5 error rate of 15.3%, far outperforming previous methods. Here are the key innovations:\n",
    "\n",
    "### Key Architectural Innovations in AlexNet\n",
    "\n",
    "1. **ReLU Activation Function:**\n",
    "   - **Innovation:** ReLU (Rectified Linear Unit) activation function replaced traditional activation functions like sigmoid or tanh.\n",
    "   - **Benefit:** ReLU helps mitigate the vanishing gradient problem, allowing deeper networks to be trained more effectively. It also accelerates the convergence of the training process, making it faster compared to sigmoid or tanh.\n",
    "\n",
    "2. **GPU Utilization:**\n",
    "   - **Innovation:** AlexNet leveraged GPU (Graphical Processing Unit) acceleration for training.\n",
    "   - **Benefit:** Utilizing GPUs significantly sped up the training process. AlexNet was trained using two NVIDIA GTX 580 GPUs in parallel, allowing for the handling of larger models and datasets more efficiently than with CPUs alone.\n",
    "\n",
    "3. **Local Response Normalization (LRN):**\n",
    "   - **Innovation:** Introduced LRN layers after some convolutional layers.\n",
    "   - **Benefit:** LRN helps improve the generalization of the model by normalizing the activations. It encourages competition for large activities among neighboring neurons, which aids in generalizing across the dataset.\n",
    "\n",
    "4. **Dropout:**\n",
    "   - **Innovation:** Employed dropout in the fully connected layers.\n",
    "   - **Benefit:** Dropout is a regularization technique that helps prevent overfitting by randomly setting a fraction of input units to zero at each update during training. This encourages the network to develop redundant representations and thus improve its robustness.\n",
    "\n",
    "5. **Data Augmentation:**\n",
    "   - **Innovation:** Used extensive data augmentation techniques.\n",
    "   - **Benefit:** Data augmentation techniques such as image translations, horizontal reflections, and patch extractions artificially increase the size of the training set. This helps improve the generalization capability of the model and reduces overfitting.\n",
    "\n",
    "6. **Overlapping Pooling:**\n",
    "   - **Innovation:** Used overlapping max-pooling layers instead of non-overlapping ones.\n",
    "   - **Benefit:** Overlapping pooling layers (with a pooling size of 3x3 and a stride of 2) help reduce the spatial dimensions more effectively while preserving more information. This results in better performance compared to non-overlapping pooling layers.\n",
    "\n",
    "7. **Deep Architecture:**\n",
    "   - **Innovation:** Increased the depth and width of the network compared to previous models.\n",
    "   - **Benefit:** The deeper and wider architecture allowed the model to learn more complex and hierarchical features from the input data. This increased the model's capacity to handle the complexity of the ImageNet dataset.\n",
    "\n",
    "### Summary of Benefits\n",
    "\n",
    "1. **ReLU Activation Function:** Faster training and mitigation of vanishing gradients.\n",
    "2. **GPU Utilization:** Efficient handling of large models and datasets, leading to faster training.\n",
    "3. **Local Response Normalization:** Improved generalization through competitive normalization.\n",
    "4. **Dropout:** Reduced overfitting and improved model robustness.\n",
    "5. **Data Augmentation:** Enhanced generalization by artificially increasing training data.\n",
    "6. **Overlapping Pooling:** Better spatial dimension reduction while preserving important features.\n",
    "7. **Deep Architecture:** Greater capacity to learn complex features and handle large-scale datasets.\n",
    "\n",
    "### Impact\n",
    "\n",
    "These innovations collectively contributed to AlexNet's groundbreaking performance in the ILSVRC 2012. The use of ReLU and dropout, in particular, has become standard practice in modern deep learning models. AlexNet's success demonstrated the potential of deep CNNs and influenced the design of subsequent architectures such as VGG, GoogLeNet, and ResNet, marking a significant milestone in the field of computer vision and deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14515b0-732c-4288-89bf-ed09c85dab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 3\n",
    "   \n",
    "In AlexNet, convolutional layers, pooling layers, and fully connected layers play distinct but complementary roles in the overall architecture. Here’s a detailed discussion of each type of layer and its contribution to the model:\n",
    "\n",
    "### 1. Convolutional Layers\n",
    "\n",
    "#### Role:\n",
    "- **Feature Extraction:** Convolutional layers are primarily responsible for extracting hierarchical features from the input image. They apply convolution operations using filters (kernels) that slide over the input, capturing spatial hierarchies and patterns such as edges, textures, and shapes.\n",
    "\n",
    "#### Key Characteristics:\n",
    "- **Filters:** AlexNet uses multiple convolutional layers, each with a varying number of filters. The first layer has 96 filters, the second has 256 filters, and so on, allowing the network to learn a rich set of features at different levels of abstraction.\n",
    "- **Activation Function:** The ReLU (Rectified Linear Unit) activation function is applied after each convolution operation. This introduces non-linearity into the model, enabling it to learn more complex functions.\n",
    "- **Strides and Padding:** Strides (the step size for the sliding filter) and padding (adding zeros around the input) are used to control the spatial dimensions of the output feature maps. This helps maintain the spatial structure while reducing dimensions progressively.\n",
    "\n",
    "#### Contribution:\n",
    "- **Hierarchical Learning:** As data passes through the convolutional layers, the network learns increasingly abstract representations. Early layers detect simple features (like edges), while deeper layers identify more complex structures (like parts of objects).\n",
    "- **Spatial Invariance:** The convolution operation allows the model to recognize features regardless of their position in the image, providing translation invariance.\n",
    "\n",
    "### 2. Pooling Layers\n",
    "\n",
    "#### Role:\n",
    "- **Downsampling:** Pooling layers reduce the spatial dimensions of the feature maps produced by convolutional layers. This helps decrease computational complexity and prevents overfitting by providing a form of regularization.\n",
    "\n",
    "#### Key Characteristics:\n",
    "- **Type of Pooling:** AlexNet primarily uses max pooling, where the maximum value from a defined window (e.g., 3x3) is selected. This retains the most salient features while discarding less significant information.\n",
    "- **Stride:** Pooling layers have strides (e.g., a stride of 2) that dictate how much the pooling window moves across the feature map, further reducing dimensions.\n",
    "\n",
    "#### Contribution:\n",
    "- **Feature Extraction Efficiency:** By downsampling the feature maps, pooling layers help condense the information, making it more manageable for the fully connected layers that follow.\n",
    "- **Robustness to Disturbances:** Pooling introduces some translation invariance, making the network more robust to slight variations in input (e.g., small translations or distortions).\n",
    "- **Control Overfitting:** By reducing the number of parameters and computations in the model, pooling helps prevent overfitting, allowing the network to generalize better to unseen data.\n",
    "\n",
    "### 3. Fully Connected Layers\n",
    "\n",
    "#### Role:\n",
    "- **Integration and Classification:** Fully connected layers (also known as dense layers) take the high-level features extracted by the convolutional and pooling layers and integrate them to make the final classification decision.\n",
    "\n",
    "#### Key Characteristics:\n",
    "- **Neurons:** AlexNet features several fully connected layers (e.g., FC6, FC7), with each layer containing a large number of neurons (4096 in FC6 and FC7).\n",
    "- **Activation Function:** ReLU is also applied in fully connected layers, allowing the network to learn complex relationships between the high-level features.\n",
    "\n",
    "#### Contribution:\n",
    "- **Final Decision Making:** The last fully connected layer (FC8) outputs class scores for the different categories in the classification task (e.g., 1000 classes for ImageNet). The softmax activation function is used to convert these scores into probabilities.\n",
    "- **High-Level Feature Learning:** The fully connected layers allow the model to learn how to combine the various features extracted from previous layers to make informed predictions. This enables the model to capture complex relationships in the data.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Convolutional Layers:** Extract local and hierarchical features from input images, enabling the model to learn spatial hierarchies.\n",
    "- **Pooling Layers:** Downsample the feature maps, reducing computational complexity while providing translation invariance and helping to prevent overfitting.\n",
    "- **Fully Connected Layers:** Integrate the learned features to make final classification decisions, leveraging the high-level representations learned by previous layers.\n",
    "\n",
    "Together, these layers form a powerful architecture that enables AlexNet to perform effectively on complex image classification tasks, demonstrating the strengths of convolutional neural networks in computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd56059-0999-4303-84af-a290ff4f45bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #Answer: 4\n",
    "   \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the AlexNet model\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0)\n",
    "        self.conv2 = nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv3 = nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(384, 384, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(384, 256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256 * 6 * 6, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 4096)\n",
    "        self.fc3 = nn.Linear(4096, 10)  # 10 classes for CIFAR-10\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.relu(self.conv5(x))\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define transformations for the CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224 for AlexNet\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Instantiate the model, define the loss function, and the optimizer\n",
    "model = AlexNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training function\n",
    "def train(model, device, trainloader, optimizer, criterion, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(trainloader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(trainloader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(trainloader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "# Testing function\n",
    "def test(model, device, testloader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    accuracy = 100. * correct / len(testloader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(testloader.dataset)} '\n",
    "          f'({accuracy:.2f}%)\\n')\n",
    "\n",
    "# Train and test the model\n",
    "epochs = 10\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, trainloader, optimizer, criterion, epoch)\n",
    "    test(model, device, testloader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
